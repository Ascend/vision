# torchvision_npu 仓库适配指导

## 一、适配原则

- torchvision_npu为torchvision在NPU硬件场景下适配框架。
一方面该框架提供torchvision所实现算子的NPU实现，另一方面该框架可以借助NPU进行图像处理功能以提供加速。
- torchvision的适配原则如下：
  1. 非侵入式修改，在适配框架中修改而不是直接在原仓库中进行修改
  2. 尽可能做到上层无感，使得调用方在尽可能少修改代码的情况下能够适配

## 二、图像处理后端支持扩展

原生torchvision在区别图像处理后端时，使用了`image_backend`后端参数来控制图像处理所用后端。

```python
# torchvision_npu/__init__.py

_image_backend = "PIL"

def set_image_backend(backend):
    global _image_backend
    if backend not in ["PIL", "accimage", "npu"]:
        raise ValueError(f"Invalid backend '{backend}'. Options are 'npu', 'PIL' and 'accimage'")
    _image_backend = backend


def get_image_backend():
    return _image_backend
```

## 三、图像读取适配

1. `default_loader`适配。

    在 `folder.py` 中重新定义 `default_loader`
    ```python
    # torchvision_npu/datasets/folder.py
    
    def default_loader(path: str) -> Any:
        from torchvision import get_image_backend
    
        if get_image_backend() == 'npu':
            return npu_loader(path)
        elif get_image_backend() == "accimage":
            return fold.accimage_loader(path)
        else:
            return fold.pil_loader(path)
    ```
   
    monky-patch原有实现
    ```python
    # torchvision_npu/datasets/folder.py
    def add_dataset_imagefolder():
        torchvision.__name__ = 'torchvision_npu'
        torchvision._image_backend = 'npu'
        torchvision.datasets.folder.default_loader = default_loader
    ```

2. 通过monky-patch数据集的`__get_item__`方法实现。

## 四、图像处理适配

torchvision在`torchvision/transforms`路径下提供了图像处理的一些基本方法，其目录结构如下：

   ```shell
   transforms/
        ├── _functional_pil.py
        ├── functional.py               # 屏蔽后端实现，将调用转发到PIL或者tensor
        ├── _functional_tensor.py
        ├── transforms.py               # 对外提供接口，调用functional.py
        └── ...
   ```

通过简单分析能够发现，transforms部分设计逻辑便为：
`transforms.py`是对外呈现的表现层，调用`torchvision.transforms.functional`中定义的函数，
`torchvision.transforms.functional`是具体实现的分发层，
将调用分发到具体实现的`torchvision.transforms.functional_xx`模块中。

由于原生分层较为清晰，故在适配transforms部分时也较为简单。
首先在`torchvision_npu/transforms`文件夹下增加`functional_npu.py`，定义需要适配的图像处理的npu实现；
然后重写并monky-patch部分需要适配的`functional.py`内容，非必须情况下，对外呈现层`transforms.py`不做monkey-patch。

例如：

```python
# torchvision/transforms/transforms.py

class Normalize(torch.nn.Module):

    def __init__(self, mean, std, inplace=False):
        super().__init__()
        self.mean = mean
        self.std = std
        self.inplace = inplace

    def forward(self, tensor: Tensor) -> Tensor:
        return F.normalize(tensor, self.mean, self.std, self.inplace)

    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)
```

```python
# torchvision_npu/transforms/functional.py

from torchvision.transforms import _functional_tensor as F_t
from torchvision.transforms import _functional_pil as F_pil
from . import functional_npu as F_npu

def normalize(tensor: Tensor, mean: List[float], std: List[float], inplace: bool = False) -> Tensor:
    if not torch.jit.is_scripting() and not torch.jit.is_tracing():
        _log_api_usage_once(normalize)
    if not isinstance(tensor, torch.Tensor):
        raise TypeError(f"img should be Tensor Image. Got {type(tensor)}")


    if tensor.device.type == 'npu':
        return F_npu.normalize(tensor, mean, std, inplace)

    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)

```

```python
# torchvision_npu/transforms/functional_npu.py

def normalize(tensor: Tensor, mean: List[float], std: List[float], inplace: bool = False) -> Tensor:
    mean = torch.as_tensor(mean).npu(non_blocking=True)
    std = torch.as_tensor(std).npu(non_blocking=True)
    if mean.ndim == 1:
        mean = mean.view(1, -1, 1, 1)
    if std.ndim == 1:
        std = std.view(1, -1, 1, 1)
    if not inplace:
        return torch_npu.image_normalize(tensor, mean, std, 0)
    return torch_npu.image_normalize_(tensor, mean, std, 0)
```

## 五、算子适配

**表 2**  NPU支持算子列表

| 算子            | 是否支持 |
|---------------|------|
| nms           | √    |
| deform_conv2d | -    |
| ps_roi_align  | -    |
| ps_roi_pool   | -    |
| roi_align     | -    |
| roi_pool      | -    |

原生torchvision的算子适配是将cuda/cpu算子以C++语言的方式进行编写，编译为so库；在加载torchvision库的时候用`torch.ops.load_library(lib_path)`载入该so库;在做torchvision_npu的适配时，考虑沿用原生实现，增加npu实现方案。

1. 原生实现分析

    torchvision关于算子完整框架代码存在于`torchvision/csrc/ops`目录下，其目录结构如下：
    ```
    torchvision/csrc/ops
                    │  nms.cpp
                    │  nms.h
                    │  ...
                    │
                    ├─autocast
                    │      nms_kernel.cpp
                    │      ...
                    │
                    ├─autograd
                    │      ...
                    │
                    ├─cpu
                    │      nms_kernel.cpp
                    │      ...
                    │
                    └─cuda
                            cuda_helpers.h
                            nms_kernel.cu
                            ...
    ```

    下面以nms算子为例分析完整的实现逻辑。

    如下代码所示，`torchvision/csrc/ops/nms.cpp`中定义了算子分发逻辑,算子作为插件附加到torch.ops.torchvision.nms中

    ```C++
    // torchvision/csrc/ops/nms.cpp

    namespace vision {
    namespace ops {

    at::Tensor nms(
        const at::Tensor& dets,
        const at::Tensor& scores,
        double iou_threshold) {
    static auto op = c10::Dispatcher::singleton()
                        .findSchemaOrThrow("torchvision::nms", "")
                        .typed<decltype(nms)>();
    return op.call(dets, scores, iou_threshold);
    }  

    TORCH_LIBRARY_FRAGMENT(torchvision, m) {
    m.def(TORCH_SELECTIVE_SCHEMA(
        "torchvision::nms(Tensor dets, Tensor scores, float iou_threshold) -> Tensor"));
    }   // 此处定义 torch.ops.torchvision.nms ，并且该函数的入参和返回值

    } // namespace ops
    } // namespace vision
    ```
   
    再以CPU实现为例分析算子侧实现，CUDA算子实现同理。
    文件`torchvision/csrc/ops/cpu/nms_kernel.cpp`的`nms_kernel`函数中定义了nms算子的CPU实现，
    后通过`TORCH_LIBRARY_IMPL(torchvision, CPU, m)`宏确认nms算子的CPU实现。

    ```C++
    // torchvision/csrc/ops/cpu/nms_kernel.cpp

    at::Tensor nms_kernel(
    const at::Tensor& dets,
    const at::Tensor& scores,
    double iou_threshold) {
        ...
    }

    TORCH_LIBRARY_IMPL(torchvision, CPU, m) {   //绑定torch.ops.torchvision.nms的CPU实现为nms_kernel
    m.impl(TORCH_SELECTIVE_NAME("torchvision::nms"), TORCH_FN(nms_kernel));
    }

    ```

    torchvision接口调用对外呈现为`torchvision.ops.nms`，实际上时在内层又是调用了`torch.ops.torchvision.nms`，其调用关系如下所示：

    ![](figs/torchvision算子示意图.PNG)



2. 适配分析

    在torchvision_npu库中对算子进行适配时其实现较为简单，具体而言分为以下几点：

    1. 在`torchvision_npu/csrc/ops/npu`文件夹下新增npu算子实现，如`nms_kernel.cpp`，示例代码如下：
        ```C++
        namespace vision {
        namespace ops {

        at::Tensor nms_kernel(
            const at::Tensor& dets,
            const at::Tensor& scores,
            double iou_threshold) {
            // 具体实现
        }

        TORCH_LIBRARY_IMPL(torchvision, PrivateUse1, m) {   //绑定torch.ops.torchvision.nms的NPU实现为nms_kernel
        m.impl(TORCH_SELECTIVE_NAME("torchvision::nms"), TORCH_FN(nms_kernel));
        }

        } // namespace ops
        } // namespace vision

        ```

    2. 在打包文件setup.py中配置ext_modules参数，将`torchvision_npu/csrc/ops/npu/*.cpp`编译至`torchvision_npu._C`模块中

    3. 在torchvision_npu的init函数中，使用`torch.ops.load_library(lib_path)`将`torchvision_npu._C.so`加载至torch框架

    4. 在NPU设备执行完成`import torchvision`和`import torchvision_npu`后，调用 `torchvision.ops.nms` 或 `torch.ops.torchvision.nms` 便可自动调用到NPU实现的nms算子

    ![](figs/torchvision_npu算子适配示意图.PNG)


3. 适配注意事项

    1. torchvision需要使用在npu设备编译的whl包，否则在NPU上，torch将无法正常加载`torchvision._C.so`和`torchvision_npu._C.so`这两个二进制库。
